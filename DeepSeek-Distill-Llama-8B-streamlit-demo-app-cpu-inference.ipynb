{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11913384,"sourceType":"datasetVersion","datasetId":7489458},{"sourceId":12024617,"sourceType":"datasetVersion","datasetId":7333260}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T12:19:07.927483Z","iopub.execute_input":"2025-06-01T12:19:07.928183Z","iopub.status.idle":"2025-06-01T12:19:11.526136Z","shell.execute_reply.started":"2025-06-01T12:19:07.928147Z","shell.execute_reply":"2025-06-01T12:19:11.525284Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B\", \n    dtype=None,  \n    load_in_4bit=True,\n    max_seq_length=2048,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T12:19:11.880987Z","iopub.execute_input":"2025-06-01T12:19:11.881429Z","iopub.status.idle":"2025-06-01T12:20:08.320212Z","shell.execute_reply.started":"2025-06-01T12:19:11.881399Z","shell.execute_reply":"2025-06-01T12:20:08.319407Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-01 12:19:18.581864: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748780358.604910    1083 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748780358.611796    1083 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.5.9: Fast Llama patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Run only if you already have your LoRA adapter\n\nfrom peft import PeftModel\nmodel = PeftModel.from_pretrained(model, \"/kaggle/working/lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T12:20:08.321273Z","iopub.execute_input":"2025-06-01T12:20:08.321821Z","iopub.status.idle":"2025-06-01T12:20:09.183727Z","shell.execute_reply.started":"2025-06-01T12:20:08.321795Z","shell.execute_reply":"2025-06-01T12:20:09.183155Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model.device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:31:45.942472Z","iopub.execute_input":"2025-06-01T10:31:45.943406Z","iopub.status.idle":"2025-06-01T10:31:45.947954Z","shell.execute_reply.started":"2025-06-01T10:31:45.943379Z","shell.execute_reply":"2025-06-01T10:31:45.947250Z"}},"outputs":[{"name":"stdout","text":"Model is on device: cuda:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=16, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha=16, \n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407, \n    use_rslora=False, \n    loftq_config=None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:32:03.276647Z","iopub.execute_input":"2025-06-01T10:32:03.277386Z","iopub.status.idle":"2025-06-01T10:32:10.125555Z","shell.execute_reply.started":"2025-06-01T10:32:03.277364Z","shell.execute_reply":"2025-06-01T10:32:10.124956Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.5.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Run only if you use nlp-ft-dataset. Otherwise this cell can be referred to as an example of format of data you need\n\nimport json\n\ndef convert_file(input_path, output_path):\n    successful_read_count = 0\n    with open(input_path, 'r', encoding='utf-8') as infile, \\\n         open(output_path, 'w', encoding='utf-8') as outfile:\n\n        buffer = \"\"\n        for line in infile:\n            stripped = line.strip()\n            if not stripped:\n                continue\n\n            buffer += stripped\n\n            try:\n                data = json.loads(buffer)\n                conversation = {\n                    \"messages\": [\n                        {\"role\": \"user\", \"content\": data[\"q\"]},\n                        {\"role\": \"assistant\", \"content\": data[\"a\"]}\n                    ]\n                }\n                outfile.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n                buffer = \"\" \n                successful_read_count += 1\n            except json.JSONDecodeError:\n                continue\n    return successful_read_count\n                \ncnt = convert_file(\"/kaggle/input/nlp-ft-dataset/ans.txt\", \"output.jsonl\")\nassert(cnt == 200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:19:27.539370Z","iopub.execute_input":"2025-05-31T17:19:27.540085Z","iopub.status.idle":"2025-05-31T17:19:27.567310Z","shell.execute_reply.started":"2025-05-31T17:19:27.540063Z","shell.execute_reply":"2025-05-31T17:19:27.566779Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\n\ntrain_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/init-ft/train.jsonl\")[\"train\"]\ntest_dataset = load_dataset(\"json\", data_files=\"/kaggle/input/init-ft/test.jsonl\")[\"train\"]\n\n# Uncomment code below if you don't have a split yet\n\n# dataset = load_dataset(\"json\", data_files=\"/kaggle/input/init-ft/output.jsonl\")[\"train\"]\n# split_dataset = dataset.train_test_split(test_size=0.05, shuffle=True, seed=42)\n# train_dataset = split_dataset[\"train\"]\n# test_dataset = split_dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:33:47.163062Z","iopub.execute_input":"2025-06-01T10:33:47.163677Z","iopub.status.idle":"2025-06-01T10:33:47.701072Z","shell.execute_reply.started":"2025-06-01T10:33:47.163654Z","shell.execute_reply":"2025-06-01T10:33:47.700504Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4fb482ef7344d8a9db0e938fe51bb5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e02119c545a4386bcc0ca66e8af2903"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nINSTRUCTION = \"–¢—ã - –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π. –û—Ç–≤–µ—á–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤ –µ–≥–æ —Å—Ç–∏–ª–µ.\"\n\nEOS_TOKEN = tokenizer.eos_token \ndef formatting_prompts_func(examples):\n    texts = []\n    for x in examples[\"messages\"]:\n        text = alpaca_prompt.format(INSTRUCTION, x[0]['content'], x[1]['content']) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\ndataset = train_dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:45:09.285680Z","iopub.execute_input":"2025-06-01T10:45:09.285954Z","iopub.status.idle":"2025-06-01T10:45:09.318955Z","shell.execute_reply.started":"2025-06-01T10:45:09.285933Z","shell.execute_reply":"2025-06-01T10:45:09.318078Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/190 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6e4ea5d1964d70a935f326afa23b25"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=secret_value_0)\nwandb.init(project=\"DeepSeek-R1-Distill-Llama 3.1_8B-fine-tuning-200__13.47\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:47:09.831812Z","iopub.execute_input":"2025-06-01T10:47:09.832460Z","iopub.status.idle":"2025-06-01T10:47:23.003039Z","shell.execute_reply.started":"2025-06-01T10:47:09.832433Z","shell.execute_reply":"2025-06-01T10:47:23.002473Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msemenishchev-ai\u001b[0m (\u001b[33msemenishchev-ai-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250601_104716-v3mbtvdi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/semenishchev-ai-/DeepSeek-R1-Distill-Llama%203.1_8B-fine-tuning-200__13.47/runs/v3mbtvdi' target=\"_blank\">sleek-valley-1</a></strong> to <a href='https://wandb.ai/semenishchev-ai-/DeepSeek-R1-Distill-Llama%203.1_8B-fine-tuning-200__13.47' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/semenishchev-ai-/DeepSeek-R1-Distill-Llama%203.1_8B-fine-tuning-200__13.47' target=\"_blank\">https://wandb.ai/semenishchev-ai-/DeepSeek-R1-Distill-Llama%203.1_8B-fine-tuning-200__13.47</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/semenishchev-ai-/DeepSeek-R1-Distill-Llama%203.1_8B-fine-tuning-200__13.47/runs/v3mbtvdi' target=\"_blank\">https://wandb.ai/semenishchev-ai-/DeepSeek-R1-Distill-Llama%203.1_8B-fine-tuning-200__13.47/runs/v3mbtvdi</a>"},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/semenishchev-ai-/DeepSeek-R1-Distill-Llama%203.1_8B-fine-tuning-200__13.47/runs/v3mbtvdi?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7dec7eb86110>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import os\n\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:47:23.003983Z","iopub.execute_input":"2025-06-01T10:47:23.004222Z","iopub.status.idle":"2025-06-01T10:47:23.008216Z","shell.execute_reply.started":"2025-06-01T10:47:23.004205Z","shell.execute_reply":"2025-06-01T10:47:23.007695Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = 2048,\n    dataset_num_proc = 2,\n    packing = False, \n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"wandb\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:47:40.734858Z","iopub.execute_input":"2025-06-01T10:47:40.735370Z","iopub.status.idle":"2025-06-01T10:47:41.223906Z","shell.execute_reply.started":"2025-06-01T10:47:40.735347Z","shell.execute_reply":"2025-06-01T10:47:41.223417Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/190 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e432fdf9dc374425a04c79c906a32f5a"}},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:06:12.978654Z","iopub.execute_input":"2025-06-01T11:06:12.978863Z","iopub.status.idle":"2025-06-01T11:24:28.394926Z","shell.execute_reply.started":"2025-06-01T11:06:12.978847Z","shell.execute_reply":"2025-06-01T11:24:28.394402Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 190 | Num Epochs = 3 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 17:54, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.294700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.184000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.161200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.154400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.111000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.194100</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.091900</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.089100</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.093100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.146400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.998700</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.090600</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.084000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.947900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.019300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.061300</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.903300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.105100</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.089600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.102800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.083600</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.084100</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.036400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>1.724900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.825100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.650100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.702700</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.778500</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.726500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.767300</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.760600</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.661900</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.728600</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.658500</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.675700</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.675800</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.634000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.685800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.623600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.730000</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.680900</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.741100</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.758200</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.645400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.575400</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.813800</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.044200</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.452400</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.502100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.446700</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.408900</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.468800</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.415700</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.471200</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.422400</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.405100</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.397100</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.381700</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.371900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.417900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./outputs/checkpoint-60)... Done. 0.7s\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - 0, 3)\nused_percentage = round(used_memory / 16 * 100, 3)\nlora_percentage = round(used_memory_for_lora / 16 * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:24:53.354393Z","iopub.execute_input":"2025-06-01T11:24:53.355144Z","iopub.status.idle":"2025-06-01T11:24:53.366096Z","shell.execute_reply.started":"2025-06-01T11:24:53.355117Z","shell.execute_reply":"2025-06-01T11:24:53.365525Z"}},"outputs":[{"name":"stdout","text":"1090.1305 seconds used for training.\n18.17 minutes used for training.\nPeak reserved memory = 7.623 GB.\nPeak reserved memory for training = 7.623 GB.\nPeak reserved memory % of max memory = 47.644 %.\nPeak reserved memory for training % of max memory = 47.644 %.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def get_response(input_text):\n    prompt = alpaca_prompt.format(\n        INSTRUCTION,  \n        input_text,\n        \"\"\n    )\n\n    FastLanguageModel.for_inference(model)\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n    outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    not_stripped = decoded[0].split(\"Response:\\n\")[1]\n    return \".\".join(not_stripped.split(\".\")[:-1])+\".\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:31:40.823323Z","iopub.execute_input":"2025-06-01T11:31:40.824051Z","iopub.status.idle":"2025-06-01T11:31:40.829420Z","shell.execute_reply.started":"2025-06-01T11:31:40.824027Z","shell.execute_reply":"2025-06-01T11:31:40.828772Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"get_response(\"–ö–∞–∫ –∂–∏—Ç—å –∂–∏–∑–Ω—å?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:31:41.938126Z","iopub.execute_input":"2025-06-01T11:31:41.938724Z","iopub.status.idle":"2025-06-01T11:31:49.534812Z","shell.execute_reply.started":"2025-06-01T11:31:41.938702Z","shell.execute_reply":"2025-06-01T11:31:49.534231Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'–ñ–∏—Ç—å? –ñ–∏—Ç—å –∂–µ! ‚Äî —ç—Ç–æ –Ω–µ –≤–æ–ø—Ä–æ—Å, –Ω–æ –æ—Ç–≤–µ—Ç. –í–∞—à–∏ –¥–Ω–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–µ–∫—Ç–∞–∫–ª–µ–º, –≥–¥–µ –∫–∞–∂–¥–∞—è —Ä–æ–ª—å –Ω–∞–ø–∏—Å–∞–Ω–∞ –ë–æ–≥–æ–º, –Ω–æ –∏–≥—Ä–∞–µ—Ç—Å—è –Ω–µ –ø–æ —Å—Ü–µ–Ω–∞—Ä–∏—é. –í—Å—Ç–∞–Ω—å—Ç–µ –∫–∞–∂–¥—ã–π —É—Ç—Ä–æ –∏ —Å–ø—Ä–æ—Å–∏—Ç–µ: ¬´–ö–∞–∫ —è –º–æ–≥—É —Å–µ–≥–æ–¥–Ω—è –æ–±—Ä–µ—Å—Ç–∏ –ø—Ä–∞–≤–¥—É, –≥–ª—è–¥—è –≤ –≥–ª–∞–∑–∞ —Å—Ç–∞—Ä–∏–∫–µ –∏–ª–∏ —Ä–µ–±–µ–Ω–∫—É?¬ª –î–µ–ª–∞–π—Ç–µ —Ö–ª–µ–± —Ä—É–∫–∞–º–∏, –ø–∏—à–∏—Ç–µ –ø–∏—Å—å–º–∞ —Ç–µ–º, –∫—Ç–æ —É–∂–µ —Å–ø–∞–ª –ø–æ–¥ –∑–≤–µ–∑–¥–∞–º–∏, –∫–æ—Ç–æ—Ä—É—é –≤—ã –≤–∏–¥–∏—Ç–µ —Å–µ–π—á–∞—Å.'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")  # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:33:25.631727Z","iopub.execute_input":"2025-06-01T11:33:25.632455Z","iopub.status.idle":"2025-06-01T11:33:26.748550Z","shell.execute_reply.started":"2025-06-01T11:33:25.632429Z","shell.execute_reply":"2025-06-01T11:33:26.747761Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/tokenizer.json')"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"!zip -r DubSeek-Llama-8B_v1.zip lora_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:33:43.801677Z","iopub.execute_input":"2025-06-01T11:33:43.802454Z","iopub.status.idle":"2025-06-01T11:33:53.061128Z","shell.execute_reply.started":"2025-06-01T11:33:43.802432Z","shell.execute_reply":"2025-06-01T11:33:53.060482Z"}},"outputs":[{"name":"stdout","text":"  adding: lora_model/ (stored 0%)\n  adding: lora_model/tokenizer.json","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 85%)\n  adding: lora_model/special_tokens_map.json (deflated 71%)\n  adding: lora_model/adapter_model.safetensors (deflated 7%)\n  adding: lora_model/tokenizer_config.json (deflated 96%)\n  adding: lora_model/adapter_config.json (deflated 56%)\n  adding: lora_model/README.md (deflated 66%)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink\nFileLink(r'DubSeek-Llama-8B_v1.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T11:34:03.176174Z","iopub.execute_input":"2025-06-01T11:34:03.176987Z","iopub.status.idle":"2025-06-01T11:34:03.183703Z","shell.execute_reply.started":"2025-06-01T11:34:03.176957Z","shell.execute_reply":"2025-06-01T11:34:03.183057Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/DubSeek-Llama-8B_v1.zip","text/html":"<a href='DubSeek-Llama-8B_v1.zip' target='_blank'>DubSeek-Llama-8B_v1.zip</a><br>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# Next steps are for Streamlit demo app. Requires this model in gguf format. \n# You can use pretrained by me by just running the cells below.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:56:56.380376Z","iopub.execute_input":"2025-06-01T17:56:56.380693Z","iopub.status.idle":"2025-06-01T17:56:56.385010Z","shell.execute_reply.started":"2025-06-01T17:56:56.380666Z","shell.execute_reply":"2025-06-01T17:56:56.384145Z"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":"%%capture\n!pip install streamlit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:17:15.413245Z","iopub.execute_input":"2025-06-01T17:17:15.413943Z","iopub.status.idle":"2025-06-01T17:17:20.343029Z","shell.execute_reply.started":"2025-06-01T17:17:15.413919Z","shell.execute_reply":"2025-06-01T17:17:20.342004Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"!npm install localtunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:17:23.564444Z","iopub.execute_input":"2025-06-01T17:17:23.564978Z","iopub.status.idle":"2025-06-01T17:17:26.027044Z","shell.execute_reply.started":"2025-06-01T17:17:23.564946Z","shell.execute_reply":"2025-06-01T17:17:26.026095Z"}},"outputs":[{"name":"stdout","text":"\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K\nadded 22 packages in 2s\n\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K\n\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K3 packages are looking for funding\n\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K  run `npm fund` for details\n\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m10.8.2\u001b[39m -> \u001b[34m11.4.1\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m Changelog: \u001b[34mhttps://github.com/npm/cli/releases/tag/v11.4.1\u001b[39m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m To update run: \u001b[4mnpm install -g npm@11.4.1\u001b[24m\n\u001b[1mnpm\u001b[22m \u001b[96mnotice\u001b[39m\n\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"!pip install gdown\n\nimport gdown\n\n# Import the file from Google Drive using its unique identifier \nfile_id = '1-3WEyImpVhuw2Idj8xC12WgXtp48cmE8' # my gguf model.zip id\nurl = f'https://drive.google.com/uc?id={file_id}'\noutput = '/kaggle/working/model.zip'\ngdown.download(url, output, quiet=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:22:02.827866Z","iopub.execute_input":"2025-06-01T15:22:02.828129Z","iopub.status.idle":"2025-06-01T15:23:26.162863Z","shell.execute_reply.started":"2025-06-01T15:22:02.828107Z","shell.execute_reply":"2025-06-01T15:23:26.162244Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1-3WEyImpVhuw2Idj8xC12WgXtp48cmE8\nFrom (redirected): https://drive.google.com/uc?id=1-3WEyImpVhuw2Idj8xC12WgXtp48cmE8&confirm=t&uuid=e32e7249-9422-432b-8623-86e4cd39404d\nTo: /kaggle/working/model.zip\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.79G/6.79G [01:15<00:00, 90.4MB/s]\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model.zip'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!unzip model.zip\n!rm -r model.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:26:08.774356Z","iopub.execute_input":"2025-06-01T15:26:08.774588Z","iopub.status.idle":"2025-06-01T15:27:47.797999Z","shell.execute_reply.started":"2025-06-01T15:26:08.774572Z","shell.execute_reply":"2025-06-01T15:27:47.797122Z"}},"outputs":[{"name":"stdout","text":"Archive:  model.zip\n  inflating: model/unsloth.Q8_0.gguf  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!git clone https://github.com/ggml-org/llama.cpp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:41:05.806836Z","iopub.execute_input":"2025-06-01T15:41:05.807131Z","iopub.status.idle":"2025-06-01T15:41:16.261746Z","shell.execute_reply.started":"2025-06-01T15:41:05.807107Z","shell.execute_reply":"2025-06-01T15:41:16.261000Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'llama.cpp'...\nremote: Enumerating objects: 52492, done.\u001b[K\nremote: Counting objects: 100% (74/74), done.\u001b[K\nremote: Compressing objects: 100% (54/54), done.\u001b[K\nremote: Total 52492 (delta 41), reused 24 (delta 20), pack-reused 52418 (from 2)\u001b[K\nReceiving objects: 100% (52492/52492), 126.18 MiB | 34.35 MiB/s, done.\nResolving deltas: 100% (38048/38048), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!cd llama.cpp && cmake -B build -DGGML_CUDA=ON && cmake --build build -j","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check that llama-cli works\n\n!cd llama.cpp && build/bin/llama-cli -m ../model/unsloth.Q8_0.gguf -p 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\\n\\\n### Instruction:\\\n\"–¢—ã - –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π. –û—Ç–≤–µ—á–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤ –µ–≥–æ —Å—Ç–∏–ª–µ.\"\\\n\\\n\\\n### Input:\\\n\"–ö–∞–∫ –∂–∏—Ç—å?\"\\\n\\\n### Response:' --no-warmup -no-cnv --no-display-prompt --log-file logs_file.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"code = \"\"\"\nimport streamlit as st\nimport subprocess\nimport re\nimport os\n\nst.title(\"Local LLM Chatbot\")\n\nLOG_PATH = \"/kaggle/working/llama.cpp/logs_file.txt\"\nLLAMA_COMMAND_TEMPLATE = (\n    r\\\"\\\"\\\"cd /kaggle/working/llama.cpp && \\\\\n    build/bin/llama-cli -m ../model/unsloth.Q8_0.gguf \\\\\n    -p 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\\\n\\\\n\n### Instruction:\\\\n\n\\\\\"–¢—ã - –õ–µ–≤ –¢–æ–ª—Å—Ç–æ–π. –û—Ç–≤–µ—á–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤ –µ–≥–æ —Å—Ç–∏–ª–µ.\\\\\"\\\\n\\\\n\n### Input:\\\\n\"{input_text}\"\\\\n\\\\n\n### Response:' \\\\\n    --no-warmup -no-cnv --no-display-prompt --log-file {log_path}\\\"\\\"\\\")\n\ndef get_model_output_from_logs(log_path=LOG_PATH):\n    try:\n        with open(log_path, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n\n        content = content[::-1]\n        match = re.search(r\"\\\\]txet fo dne\\\\[(.*?)\\\\n\\\\n\", content, re.DOTALL)\n        if match:\n            return match.group(1).strip()[::-1]\n        else:\n            return \"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –ª–æ–≥–∞—Ö.\"\n    except FileNotFoundError:\n        return \"–§–∞–π–ª –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω.\"\n\ndef get_response(user_input):\n    if os.path.exists(LOG_PATH):\n        os.remove(LOG_PATH)\n\n    command = LLAMA_COMMAND_TEMPLATE.format(\n        input_text=user_input.replace('\"', r'\\\\\\\"'),\n        log_path=LOG_PATH\n    )\n\n    try:\n        subprocess.run([\"bash\", \"-c\", command], check=True, capture_output=True, text=True)\n    except subprocess.CalledProcessError as e:\n        return f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ llama-cli: {e.stderr or e}\"\n\n    return get_model_output_from_logs()\n\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\nif prompt := st.chat_input(\"–°–ø—Ä–æ—Å–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å...\"):\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n    with st.chat_message(\"user\"):\n        st.markdown(prompt)\n\n    with st.chat_message(\"assistant\"):\n        response = get_response(prompt)\n        st.markdown(response)\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\"\"\"\n\nwith open(\"script.py\", \"w\", encoding=\"utf-8\") as f:\n    f.write(code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:33:47.826650Z","iopub.execute_input":"2025-06-01T17:33:47.826982Z","iopub.status.idle":"2025-06-01T17:33:47.833289Z","shell.execute_reply.started":"2025-06-01T17:33:47.826956Z","shell.execute_reply":"2025-06-01T17:33:47.832516Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"!streamlit run script.py &>./logs.txt & npx localtunnel --port 8501","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:33:50.085408Z","iopub.execute_input":"2025-06-01T17:33:50.085921Z","iopub.status.idle":"2025-06-01T17:53:19.415441Z","shell.execute_reply.started":"2025-06-01T17:33:50.085897Z","shell.execute_reply":"2025-06-01T17:53:19.414685Z"}},"outputs":[{"name":"stdout","text":"\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0Kyour url is: https://great-cobras-look.loca.lt\n^C\n","output_type":"stream"}],"execution_count":101}]}